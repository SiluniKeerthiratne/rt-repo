{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff4f7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ccdd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision numpy matplotlib -q\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a708816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb6edd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Apple Silicon GPU (MPS)\n",
      "  Device: mps\n",
      "\n",
      "Current device: mps\n",
      "✓ Default tensors will be created on MPS\n"
     ]
    }
   ],
   "source": [
    "# Cell 2.5: Setup for Apple Silicon (M4 Pro)\n",
    "import torch\n",
    "\n",
    "# Check if MPS (Metal Performance Shaders) is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "    print(f\"  Device: {device}\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"✓ Using NVIDIA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"⚠ Using CPU (slower)\")\n",
    "\n",
    "print(f\"\\nCurrent device: {device}\")\n",
    "\n",
    "# Set default tensor type for faster operations\n",
    "if device.type == \"mps\":\n",
    "    torch.set_default_device(device)\n",
    "    print(\"✓ Default tensors will be created on MPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07d891eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Lower Transformer defined\n"
     ]
    }
   ],
   "source": [
    "class LowerTransformer(nn.Module):\n",
    "    \"\"\"Aggregates observations, actions, and rewards from target and neighbors\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, num_layers=3, n_neighbors=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        # Separate embeddings for o, a, r\n",
    "        self.obs_embed = nn.Linear(25, d_model)\n",
    "        self.action_embed = nn.Linear(8, d_model)\n",
    "        self.reward_embed = nn.Linear(1, d_model)\n",
    "        \n",
    "        # Decision token\n",
    "        self.decision_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 3*(1+n_neighbors)+1, d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, obs, actions, rewards, mask=None):\n",
    "        batch_size = obs.shape[0]\n",
    "        \n",
    "        # Embed o, a, r separately\n",
    "        obs_emb = self.obs_embed(obs)\n",
    "        act_emb = self.action_embed(actions)\n",
    "        rew_emb = self.reward_embed(rewards)\n",
    "        \n",
    "        # Concatenate along sequence dimension\n",
    "        seq = torch.stack([obs_emb, act_emb, rew_emb], dim=2)\n",
    "        seq = seq.reshape(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Prepend decision token\n",
    "        decision = self.decision_token.expand(batch_size, -1, -1)\n",
    "        seq = torch.cat([decision, seq], dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        seq = seq + self.pos_embed\n",
    "        \n",
    "        # Apply transformer\n",
    "        out = self.transformer(seq)\n",
    "        \n",
    "        return out[:, 0, :]  # Return decision token output\n",
    "\n",
    "print(\"✓ Lower Transformer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acbd1f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Upper Transformer defined\n"
     ]
    }
   ],
   "source": [
    "class UpperTransformer(nn.Module):\n",
    "    \"\"\"Learns scenario-agnostic decision policies across timesteps\"\"\"\n",
    "    def __init__(self, d_model=64, d_output=128, nhead=4, num_layers=3, history_len=10):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_output = d_output\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # Project lower transformer output\n",
    "        self.input_proj = nn.Linear(d_model, d_output)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, history_len, d_output))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_output,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_output*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, lower_outputs):\n",
    "        # Project to d_output dimension\n",
    "        x = self.input_proj(lower_outputs)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Apply transformer\n",
    "        out = self.transformer(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"✓ Upper Transformer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3722266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dynamic Predictor defined\n"
     ]
    }
   ],
   "source": [
    "class DynamicPredictor(nn.Module):\n",
    "    \"\"\"Predicts next state embedding for learning environment dynamics\"\"\"\n",
    "    def __init__(self, d_model=128, n_neighbors=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_dim = d_model + (1 + n_neighbors) * (8 + 1)\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, prev_embedding, actions, rewards):\n",
    "        batch_size = prev_embedding.shape[0]\n",
    "        \n",
    "        actions_flat = actions.reshape(batch_size, -1)\n",
    "        rewards_flat = rewards.reshape(batch_size, -1)\n",
    "        \n",
    "        x = torch.cat([prev_embedding, actions_flat, rewards_flat], dim=-1)\n",
    "        \n",
    "        return self.predictor(x)\n",
    "\n",
    "print(\"✓ Dynamic Predictor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "232374da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ X-Light model defined\n"
     ]
    }
   ],
   "source": [
    "class XLight(nn.Module):\n",
    "    \"\"\"Complete X-Light model\"\"\"\n",
    "    def __init__(self, \n",
    "                 obs_dim=25,\n",
    "                 action_dim=8,\n",
    "                 d_model=64,\n",
    "                 d_output=128,\n",
    "                 n_neighbors=4,\n",
    "                 history_len=10,\n",
    "                 nhead=4,\n",
    "                 num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # Lower Transformer\n",
    "        self.lower_transformer = LowerTransformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "        \n",
    "        # Upper Transformer\n",
    "        self.upper_transformer = UpperTransformer(\n",
    "            d_model=d_model,\n",
    "            d_output=d_output,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            history_len=history_len\n",
    "        )\n",
    "        \n",
    "        # Dynamic Predictor\n",
    "        self.dynamic_predictor = DynamicPredictor(\n",
    "            d_model=d_output,\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "        \n",
    "        # Actor (with residual connection)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(d_output + obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(d_output, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs_history, action_history, reward_history, \n",
    "                current_obs, neighbor_mask=None):\n",
    "        batch_size = obs_history.shape[0]\n",
    "        \n",
    "        # Process each timestep through Lower Transformer\n",
    "        lower_outputs = []\n",
    "        for t in range(self.history_len):\n",
    "            mask_t = neighbor_mask[:, t, :] if neighbor_mask is not None else None\n",
    "            lower_out = self.lower_transformer(\n",
    "                obs_history[:, t, :, :],\n",
    "                action_history[:, t, :, :],\n",
    "                reward_history[:, t, :, :],\n",
    "                mask_t\n",
    "            )\n",
    "            lower_outputs.append(lower_out)\n",
    "        \n",
    "        lower_outputs = torch.stack(lower_outputs, dim=1)\n",
    "        \n",
    "        # Process through Upper Transformer\n",
    "        upper_outputs = self.upper_transformer(lower_outputs)\n",
    "        \n",
    "        # Dynamic prediction for pretext task\n",
    "        predicted_embeddings = []\n",
    "        for t in range(self.history_len - 1):\n",
    "            pred = self.dynamic_predictor(\n",
    "                upper_outputs[:, t, :],\n",
    "                action_history[:, t, :, :],\n",
    "                reward_history[:, t, :, :]\n",
    "            )\n",
    "            predicted_embeddings.append(pred)\n",
    "        predicted_embeddings = torch.stack(predicted_embeddings, dim=1) if predicted_embeddings else None\n",
    "        \n",
    "        # Use last timestep for decision\n",
    "        last_embedding = upper_outputs[:, -1, :]\n",
    "        \n",
    "        # Actor with residual connection\n",
    "        actor_input = torch.cat([last_embedding, current_obs], dim=-1)\n",
    "        action_logits = self.actor(actor_input)\n",
    "        \n",
    "        # Critic\n",
    "        value = self.critic(last_embedding)\n",
    "        \n",
    "        return action_logits, value, predicted_embeddings, upper_outputs[:, 1:, :]\n",
    "\n",
    "print(\"✓ X-Light model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b074ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Traffic Environment defined (FIXED - 25 features)\n",
      "  Observation shape: (16, 25)\n",
      "  Single observation length: 25\n"
     ]
    }
   ],
   "source": [
    "class SimpleTrafficEnv:\n",
    "    \"\"\"Simplified traffic simulation environment\"\"\"\n",
    "    def __init__(self, n_intersections=16, n_neighbors=4, max_queue=50):\n",
    "        self.n_intersections = n_intersections\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_queue = max_queue\n",
    "        self.n_phases = 8\n",
    "        \n",
    "        # Create a grid topology (4x4)\n",
    "        self.grid_size = int(np.sqrt(n_intersections))\n",
    "        self.adjacency = self._create_grid_adjacency()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def _create_grid_adjacency(self):\n",
    "        \"\"\"Create adjacency matrix for grid topology\"\"\"\n",
    "        adj = np.zeros((self.n_intersections, self.n_intersections))\n",
    "        for i in range(self.n_intersections):\n",
    "            row = i // self.grid_size\n",
    "            col = i % self.grid_size\n",
    "            \n",
    "            neighbors = []\n",
    "            if row > 0:\n",
    "                neighbors.append((row-1) * self.grid_size + col)\n",
    "            if row < self.grid_size - 1:\n",
    "                neighbors.append((row+1) * self.grid_size + col)\n",
    "            if col > 0:\n",
    "                neighbors.append(row * self.grid_size + (col-1))\n",
    "            if col < self.grid_size - 1:\n",
    "                neighbors.append(row * self.grid_size + (col+1))\n",
    "            \n",
    "            for n in neighbors:\n",
    "                adj[i, n] = 1\n",
    "                \n",
    "        return adj\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.queues = np.random.randint(0, self.max_queue//2, \n",
    "                                       (self.n_intersections, 4))\n",
    "        self.current_phase = np.zeros(self.n_intersections, dtype=int)\n",
    "        self.phase_duration = np.zeros(self.n_intersections)\n",
    "        self.time_step = 0\n",
    "        \n",
    "        return self._get_all_observations()\n",
    "    \n",
    "    def _get_all_observations(self):\n",
    "        \"\"\"Get observations for all intersections\"\"\"\n",
    "        obs = []\n",
    "        for i in range(self.n_intersections):\n",
    "            obs.append(self._get_observation(i))\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "    \n",
    "    def _get_observation(self, intersection_id):\n",
    "        \"\"\"\n",
    "        Get observation for a single intersection\n",
    "        State features (25 total):\n",
    "        - Queue lengths (4)\n",
    "        - Current phase one-hot (8)\n",
    "        - Occupancy (4)\n",
    "        - Flow (4)\n",
    "        - Number of stops (4)\n",
    "        - Phase duration (1)\n",
    "        \"\"\"\n",
    "        # Queue lengths (4)\n",
    "        queues = self.queues[intersection_id] / self.max_queue\n",
    "        \n",
    "        # Current phase one-hot (8)\n",
    "        phase_onehot = np.zeros(8, dtype=np.float32)\n",
    "        phase_onehot[self.current_phase[intersection_id]] = 1\n",
    "        \n",
    "        # Occupancy (4) - simplified as normalized queues with noise\n",
    "        occupancy = np.clip(queues + np.random.randn(4) * 0.1, 0, 1).astype(np.float32)\n",
    "        \n",
    "        # Flow (4) - random for simulation\n",
    "        flow = (np.random.rand(4) * 0.5).astype(np.float32)\n",
    "        \n",
    "        # Number of stops (4) - approximated\n",
    "        num_stops = (queues > 0.3).astype(np.float32)\n",
    "        \n",
    "        # Phase duration normalized (1)\n",
    "        phase_dur = np.array([min(self.phase_duration[intersection_id] / 10.0, 1.0)], dtype=np.float32)\n",
    "        \n",
    "        # Concatenate all features: 4 + 8 + 4 + 4 + 4 + 1 = 25\n",
    "        obs = np.concatenate([queues, phase_onehot, occupancy, flow, num_stops, phase_dur])\n",
    "        \n",
    "        return obs.astype(np.float32)\n",
    "    \n",
    "    def get_neighbors(self, intersection_id):\n",
    "        \"\"\"Get neighbor IDs for an intersection\"\"\"\n",
    "        neighbors = np.where(self.adjacency[intersection_id] == 1)[0]\n",
    "        \n",
    "        if len(neighbors) < self.n_neighbors:\n",
    "            neighbors = np.pad(neighbors, (0, self.n_neighbors - len(neighbors)), \n",
    "                             constant_values=-1)\n",
    "        else:\n",
    "            neighbors = neighbors[:self.n_neighbors]\n",
    "            \n",
    "        return neighbors\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute actions for all intersections\"\"\"\n",
    "        rewards = np.zeros(self.n_intersections, dtype=np.float32)\n",
    "        \n",
    "        for i in range(self.n_intersections):\n",
    "            # Change phase if different\n",
    "            if actions[i] != self.current_phase[i]:\n",
    "                self.current_phase[i] = actions[i]\n",
    "                self.phase_duration[i] = 0\n",
    "            else:\n",
    "                self.phase_duration[i] += 1\n",
    "            \n",
    "            # Simulate traffic flow\n",
    "            active_movements = self._get_active_movements(actions[i])\n",
    "            for movement in active_movements:\n",
    "                reduction = np.random.poisson(3)\n",
    "                self.queues[i, movement] = max(0, self.queues[i, movement] - reduction)\n",
    "            \n",
    "            # Add arriving vehicles to all approaches\n",
    "            arrivals = np.random.poisson(2, 4)\n",
    "            self.queues[i] += arrivals\n",
    "            self.queues[i] = np.clip(self.queues[i], 0, self.max_queue)\n",
    "            \n",
    "            # Calculate reward (negative sum of queues and waiting time)\n",
    "            queue_penalty = -np.sum(self.queues[i])\n",
    "            wait_penalty = -np.sum(self.queues[i] ** 1.5) * 0.01\n",
    "            rewards[i] = queue_penalty + wait_penalty\n",
    "        \n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= 360  # Shorter episodes for faster training\n",
    "        \n",
    "        obs = self._get_all_observations()\n",
    "        \n",
    "        return obs, rewards, done, {}\n",
    "    \n",
    "    def _get_active_movements(self, phase):\n",
    "        \"\"\"Get which movements are active for a phase\"\"\"\n",
    "        phase_movements = {\n",
    "            0: [0, 2], 1: [1, 3], 2: [0, 1], 3: [0, 3],\n",
    "            4: [2, 1], 5: [2, 3], 6: [0], 7: [1],\n",
    "        }\n",
    "        return phase_movements.get(phase, [])\n",
    "\n",
    "print(\"✓ Traffic Environment defined (FIXED - 25 features)\")\n",
    "\n",
    "# Test the observation shape\n",
    "test_env = SimpleTrafficEnv()\n",
    "test_obs = test_env.reset()\n",
    "print(f\"  Observation shape: {test_obs.shape}\")\n",
    "print(f\"  Single observation length: {len(test_obs[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb422bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training components defined (FIXED with device)\n"
     ]
    }
   ],
   "source": [
    "class RolloutBuffer:\n",
    "    \"\"\"Store trajectories for PPO training\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        \n",
    "    def push(self, obs_history, action_history, reward_history, \n",
    "             current_obs, action, reward, value, log_prob, neighbor_indices):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "            \n",
    "        self.buffer.append({\n",
    "            'obs_history': obs_history,\n",
    "            'action_history': action_history,\n",
    "            'reward_history': reward_history,\n",
    "            'current_obs': current_obs,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'value': value,\n",
    "            'log_prob': log_prob,\n",
    "            'neighbor_indices': neighbor_indices\n",
    "        })\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def compute_ppo_loss(model, batch, device, clip_epsilon=0.2, alpha=1.0, beta=0.5, gamma=1.0):\n",
    "    \"\"\"Compute PPO loss with dynamic prediction\"\"\"\n",
    "    # Prepare batch data and move to device\n",
    "    obs_hist = torch.stack([torch.FloatTensor(t['obs_history']) for t in batch]).to(device)\n",
    "    act_hist = torch.stack([torch.FloatTensor(t['action_history']) for t in batch]).to(device)\n",
    "    rew_hist = torch.stack([torch.FloatTensor(t['reward_history']) for t in batch]).to(device)\n",
    "    current_obs = torch.stack([torch.FloatTensor(t['current_obs']) for t in batch]).to(device)\n",
    "    actions = torch.LongTensor([t['action'] for t in batch]).to(device)\n",
    "    rewards = torch.FloatTensor([t['reward'] for t in batch]).to(device)\n",
    "    old_values = torch.FloatTensor([t['value'] for t in batch]).to(device)\n",
    "    old_log_probs = torch.FloatTensor([t['log_prob'] for t in batch]).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    action_logits, values, predicted_embs, target_embs = model(\n",
    "        obs_hist, act_hist, rew_hist, current_obs\n",
    "    )\n",
    "    \n",
    "    # Actor loss (PPO)\n",
    "    dist = Categorical(logits=action_logits)\n",
    "    new_log_probs = dist.log_prob(actions)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    advantages = rewards - old_values.detach()\n",
    "    \n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # Critic loss\n",
    "    critic_loss = F.mse_loss(values.squeeze(), rewards)\n",
    "    \n",
    "    # Dynamic prediction loss\n",
    "    if predicted_embs is not None and target_embs is not None:\n",
    "        pred_loss = F.mse_loss(predicted_embs, target_embs.detach())\n",
    "    else:\n",
    "        pred_loss = torch.tensor(0.0).to(device)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = alpha * actor_loss + beta * critic_loss + gamma * pred_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'actor_loss': actor_loss.item(),\n",
    "        'critic_loss': critic_loss.item(),\n",
    "        'pred_loss': pred_loss.item() if isinstance(pred_loss, torch.Tensor) else pred_loss,\n",
    "        'entropy': entropy.item()\n",
    "    }\n",
    "\n",
    "print(\"✓ Training components defined (FIXED with device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b7dbd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Trainer class defined (FIXED with device handling)\n"
     ]
    }
   ],
   "source": [
    "class XLightTrainer:\n",
    "    \"\"\"Trainer for X-Light model\"\"\"\n",
    "    def __init__(self, model, env, lr=3e-4, history_len=10, n_neighbors=4):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.history_len = history_len\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.device = next(model.parameters()).device  # Get model's device\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.buffer = RolloutBuffer(capacity=10000)\n",
    "        \n",
    "        self.obs_histories = {}\n",
    "        self.action_histories = {}\n",
    "        self.reward_histories = {}\n",
    "        \n",
    "        self._init_histories()\n",
    "        \n",
    "        print(f\"✓ Trainer initialized on device: {self.device}\")\n",
    "    \n",
    "    def _init_histories(self):\n",
    "        \"\"\"Initialize history buffers\"\"\"\n",
    "        for i in range(self.env.n_intersections):\n",
    "            self.obs_histories[i] = deque(maxlen=self.history_len)\n",
    "            self.action_histories[i] = deque(maxlen=self.history_len)\n",
    "            self.reward_histories[i] = deque(maxlen=self.history_len)\n",
    "            \n",
    "            # Initialize with proper shape: [1 + n_neighbors, feature_dim]\n",
    "            dummy_obs = np.zeros((1 + self.n_neighbors, 25), dtype=np.float32)\n",
    "            dummy_action = np.zeros((1 + self.n_neighbors, 8), dtype=np.float32)\n",
    "            dummy_reward = np.zeros((1 + self.n_neighbors, 1), dtype=np.float32)\n",
    "            \n",
    "            for _ in range(self.history_len):\n",
    "                self.obs_histories[i].append(dummy_obs.copy())\n",
    "                self.action_histories[i].append(dummy_action.copy())\n",
    "                self.reward_histories[i].append(dummy_reward.copy())\n",
    "    \n",
    "    def _get_neighbor_data(self, intersection_id, all_obs, all_actions, all_rewards):\n",
    "        \"\"\"Get data for target and neighbors\"\"\"\n",
    "        neighbor_ids = self.env.get_neighbors(intersection_id)\n",
    "        \n",
    "        # Initialize arrays with proper shape\n",
    "        obs_batch = np.zeros((1 + self.n_neighbors, 25), dtype=np.float32)\n",
    "        action_batch = np.zeros((1 + self.n_neighbors, 8), dtype=np.float32)\n",
    "        reward_batch = np.zeros((1 + self.n_neighbors, 1), dtype=np.float32)\n",
    "        \n",
    "        # Target intersection\n",
    "        obs_batch[0] = all_obs[intersection_id]\n",
    "        action_batch[0] = all_actions[intersection_id]\n",
    "        reward_batch[0, 0] = all_rewards[intersection_id]\n",
    "        \n",
    "        # Neighbors\n",
    "        for idx, nid in enumerate(neighbor_ids):\n",
    "            if nid >= 0:  # Valid neighbor\n",
    "                obs_batch[idx + 1] = all_obs[nid]\n",
    "                action_batch[idx + 1] = all_actions[nid]\n",
    "                reward_batch[idx + 1, 0] = all_rewards[nid]\n",
    "            # else: already initialized with zeros\n",
    "        \n",
    "        return obs_batch, action_batch, reward_batch, neighbor_ids\n",
    "    \n",
    "    def train_episode(self):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        # Initialize action and reward arrays\n",
    "        all_actions_onehot = np.zeros((self.env.n_intersections, 8), dtype=np.float32)\n",
    "        all_actions_onehot[:, 0] = 1  # Start with phase 0\n",
    "        all_rewards = np.zeros(self.env.n_intersections, dtype=np.float32)\n",
    "        \n",
    "        while not done:\n",
    "            actions = []\n",
    "            values = []\n",
    "            log_probs = []\n",
    "            \n",
    "            for i in range(self.env.n_intersections):\n",
    "                # Get neighbor data\n",
    "                obs_data, action_data, reward_data, neighbor_ids = \\\n",
    "                    self._get_neighbor_data(i, obs, all_actions_onehot, all_rewards)\n",
    "                \n",
    "                # Update histories\n",
    "                self.obs_histories[i].append(obs_data.copy())\n",
    "                self.action_histories[i].append(action_data.copy())\n",
    "                self.reward_histories[i].append(reward_data.copy())\n",
    "                \n",
    "                # Convert histories to arrays with explicit shape\n",
    "                obs_hist_list = list(self.obs_histories[i])\n",
    "                act_hist_list = list(self.action_histories[i])\n",
    "                rew_hist_list = list(self.reward_histories[i])\n",
    "                \n",
    "                # Stack into arrays: [history_len, 1+n_neighbors, feature_dim]\n",
    "                obs_hist_array = np.stack(obs_hist_list, axis=0)  # [10, 5, 25]\n",
    "                act_hist_array = np.stack(act_hist_list, axis=0)  # [10, 5, 8]\n",
    "                rew_hist_array = np.stack(rew_hist_list, axis=0)  # [10, 5, 1]\n",
    "                \n",
    "                # Convert to tensors and add batch dimension - MOVE TO DEVICE\n",
    "                obs_hist = torch.FloatTensor(obs_hist_array).unsqueeze(0).to(self.device)\n",
    "                act_hist = torch.FloatTensor(act_hist_array).unsqueeze(0).to(self.device)\n",
    "                rew_hist = torch.FloatTensor(rew_hist_array).unsqueeze(0).to(self.device)\n",
    "                current_obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Get action from model\n",
    "                with torch.no_grad():\n",
    "                    action_logits, value, _, _ = self.model(\n",
    "                        obs_hist, act_hist, rew_hist, current_obs_tensor\n",
    "                    )\n",
    "                    dist = Categorical(logits=action_logits)\n",
    "                    action = dist.sample()\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                \n",
    "                # Move back to CPU for storage\n",
    "                actions.append(action.cpu().item())\n",
    "                values.append(value.cpu().item())\n",
    "                log_probs.append(log_prob.cpu().item())\n",
    "                \n",
    "                # Store in buffer (will update reward later)\n",
    "                self.buffer.push(\n",
    "                    obs_history=obs_hist_array.copy(),\n",
    "                    action_history=act_hist_array.copy(),\n",
    "                    reward_history=rew_hist_array.copy(),\n",
    "                    current_obs=obs[i].copy(),\n",
    "                    action=action.cpu().item(),\n",
    "                    reward=0.0,  # Will be updated\n",
    "                    value=value.cpu().item(),\n",
    "                    log_prob=log_prob.cpu().item(),\n",
    "                    neighbor_indices=neighbor_ids\n",
    "                )\n",
    "            \n",
    "            # Execute actions in environment\n",
    "            next_obs, rewards, done, _ = self.env.step(actions)\n",
    "            \n",
    "            # Update rewards in buffer (last n_intersections entries)\n",
    "            buffer_len = len(self.buffer)\n",
    "            for i in range(self.env.n_intersections):\n",
    "                if buffer_len > i:\n",
    "                    idx = buffer_len - self.env.n_intersections + i\n",
    "                    self.buffer.buffer[idx]['reward'] = float(rewards[i])\n",
    "            \n",
    "            # Update action one-hot encoding for next iteration\n",
    "            all_actions_onehot = np.zeros((self.env.n_intersections, 8), dtype=np.float32)\n",
    "            for i, a in enumerate(actions):\n",
    "                all_actions_onehot[i, a] = 1.0\n",
    "            \n",
    "            all_rewards = rewards.astype(np.float32)\n",
    "            obs = next_obs\n",
    "            episode_reward += np.mean(rewards)\n",
    "            step_count += 1\n",
    "        \n",
    "        return episode_reward, step_count\n",
    "    \n",
    "    def update(self, batch_size=64, n_updates=4):\n",
    "        \"\"\"Update model using PPO\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return {}\n",
    "        \n",
    "        losses = {\n",
    "            'actor_loss': [],\n",
    "            'critic_loss': [],\n",
    "            'pred_loss': [],\n",
    "            'entropy': []\n",
    "        }\n",
    "        \n",
    "        for _ in range(n_updates):\n",
    "            batch = self.buffer.sample(batch_size)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss, loss_dict = compute_ppo_loss(self.model, batch, device=self.device)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            for key in loss_dict:\n",
    "                losses[key].append(loss_dict[key])\n",
    "        \n",
    "        avg_losses = {k: np.mean(v) for k, v in losses.items()}\n",
    "        return avg_losses\n",
    "\n",
    "print(\"✓ Trainer class defined (FIXED with device handling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d65371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training function defined (optimized for M4)\n"
     ]
    }
   ],
   "source": [
    "def train_xlight(n_episodes=50, update_interval=5, plot_live=True):\n",
    "    \"\"\"Main training loop with live plotting\"\"\"\n",
    "    \n",
    "    # Create environment\n",
    "    env = SimpleTrafficEnv(n_intersections=16, n_neighbors=4)\n",
    "    \n",
    "    # Create model and move to device\n",
    "    model = XLight(\n",
    "        obs_dim=25,\n",
    "        action_dim=8,\n",
    "        d_model=64,\n",
    "        d_output=128,\n",
    "        n_neighbors=4,\n",
    "        history_len=10,\n",
    "        nhead=4,\n",
    "        num_layers=3\n",
    "    ).to(device)  # Move to MPS/GPU\n",
    "    \n",
    "    print(f\"✓ Model created on {device}\")\n",
    "    print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = XLightTrainer(model, env, lr=3e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    \n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Train episode\n",
    "        episode_reward, steps = trainer.train_episode()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Update model\n",
    "        if episode % update_interval == 0 and episode > 0:\n",
    "            losses = trainer.update(batch_size=64, n_updates=4)\n",
    "            \n",
    "            if losses:\n",
    "                actor_losses.append(losses['actor_loss'])\n",
    "                critic_losses.append(losses['critic_loss'])\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            eps_per_sec = (episode + 1) / elapsed\n",
    "            \n",
    "            print(f\"Episode {episode}/{n_episodes} | Reward: {episode_reward:.2f} | Steps: {steps} | Speed: {eps_per_sec:.2f} ep/s\")\n",
    "            if losses:\n",
    "                print(f\"  Actor Loss: {losses['actor_loss']:.4f} | Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "                print(f\"  Pred Loss: {losses['pred_loss']:.4f} | Entropy: {losses['entropy']:.4f}\")\n",
    "            \n",
    "            # Live plotting\n",
    "            if plot_live and episode > update_interval:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "                \n",
    "                # Plot rewards\n",
    "                axes[0].plot(episode_rewards, label='Episode Reward', color='blue', alpha=0.6)\n",
    "                if len(episode_rewards) >= 5:\n",
    "                    axes[0].plot(np.convolve(episode_rewards, np.ones(5)/5, mode='valid'), \n",
    "                               label='Moving Avg (5)', color='red', linewidth=2)\n",
    "                axes[0].set_xlabel('Episode')\n",
    "                axes[0].set_ylabel('Average Reward')\n",
    "                axes[0].set_title('Training Progress')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot losses\n",
    "                if actor_losses:\n",
    "                    x_loss = np.arange(len(actor_losses)) * update_interval\n",
    "                    axes[1].plot(x_loss, actor_losses, label='Actor Loss', color='orange')\n",
    "                    axes[1].plot(x_loss, critic_losses, label='Critic Loss', color='green')\n",
    "                    axes[1].set_xlabel('Episode')\n",
    "                    axes[1].set_ylabel('Loss')\n",
    "                    axes[1].set_title('Training Losses')\n",
    "                    axes[1].legend()\n",
    "                    axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nEpisode {episode}/{n_episodes}\")\n",
    "                print(f\"Latest Reward: {episode_reward:.2f}\")\n",
    "                print(f\"Average Reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "                print(f\"Training speed: {eps_per_sec:.2f} episodes/sec\")\n",
    "            \n",
    "            # Clear buffer periodically\n",
    "            if episode % (update_interval * 4) == 0:\n",
    "                trainer.buffer.clear()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Total time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"Average time per episode: {total_time/n_episodes:.2f} seconds\")\n",
    "    \n",
    "    return model, trainer, episode_rewards\n",
    "\n",
    "print(\"✓ Training function defined (optimized for M4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eefadf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying observation dimensions...\n",
      "Number of intersections: 16\n",
      "Observation array shape: (16, 25)\n",
      "Expected: (16, 25)\n",
      "Single observation shape: (25,)\n",
      "Single observation length: 25\n",
      "\n",
      "Observation breakdown:\n",
      "  Queue lengths (4): [0.08 0.24 0.18 0.4 ]\n",
      "  Phase one-hot (8): [1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  Occupancy (4): [0.1681443  0.36384606 0.24556527 0.59466654]\n",
      "  Flow (4): [0.05144801 0.03665401 0.11195619 0.46205184]\n",
      "  Num stops (4): [0. 0. 0. 1.]\n",
      "  Phase duration (1): [0.]\n",
      "  Total features: 25\n",
      "\n",
      "✓ All dimensions correct!\n"
     ]
    }
   ],
   "source": [
    "# Verify observation dimensions\n",
    "print(\"Verifying observation dimensions...\")\n",
    "test_env = SimpleTrafficEnv(n_intersections=16, n_neighbors=4)\n",
    "obs = test_env.reset()\n",
    "\n",
    "print(f\"Number of intersections: {test_env.n_intersections}\")\n",
    "print(f\"Observation array shape: {obs.shape}\")\n",
    "print(f\"Expected: ({test_env.n_intersections}, 25)\")\n",
    "print(f\"Single observation shape: {obs[0].shape}\")\n",
    "print(f\"Single observation length: {len(obs[0])}\")\n",
    "\n",
    "# Break down the observation\n",
    "sample_obs = obs[0]\n",
    "print(f\"\\nObservation breakdown:\")\n",
    "print(f\"  Queue lengths (4): {sample_obs[0:4]}\")\n",
    "print(f\"  Phase one-hot (8): {sample_obs[4:12]}\")\n",
    "print(f\"  Occupancy (4): {sample_obs[12:16]}\")\n",
    "print(f\"  Flow (4): {sample_obs[16:20]}\")\n",
    "print(f\"  Num stops (4): {sample_obs[20:24]}\")\n",
    "print(f\"  Phase duration (1): {sample_obs[24:25]}\")\n",
    "print(f\"  Total features: {len(sample_obs)}\")\n",
    "\n",
    "assert len(sample_obs) == 25, f\"Expected 25 features, got {len(sample_obs)}\"\n",
    "print(\"\\n✓ All dimensions correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15c5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model created on mps\n",
      "  Total parameters: 954,377\n",
      "✓ Trainer initialized on device: mps:0\n",
      "\n",
      "Starting training...\n",
      "==================================================\n",
      "Episode 5/50 | Reward: -67278.58 | Steps: 360 | Speed: 0.01 ep/s\n",
      "  Actor Loss: 191.7553 | Critic Loss: 37273.6973\n",
      "  Pred Loss: 8.4842 | Entropy: 2.0735\n"
     ]
    }
   ],
   "source": [
    "# RUN THIS CELL TO START TRAINING\n",
    "model, trainer, rewards = train_xlight(n_episodes=50, update_interval=5, plot_live=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, label='Episode Reward', alpha=0.6)\n",
    "plt.plot(np.convolve(rewards, np.ones(5)/5, mode='valid'), \n",
    "         label='Moving Average (5)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Training Progress - Final')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Statistics:\")\n",
    "print(f\"  Mean Reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"  Std Reward: {np.std(rewards):.2f}\")\n",
    "print(f\"  Best Reward: {np.max(rewards):.2f}\")\n",
    "print(f\"  Worst Reward: {np.min(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96314e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    \"\"\"Evaluate trained model\"\"\"\n",
    "    print(\"Evaluating model...\")\n",
    "    device = next(model.parameters()).device\n",
    "    total_rewards = []\n",
    "    n_neighbors = 4\n",
    "    history_len = 10\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Initialize histories\n",
    "        obs_histories = {}\n",
    "        action_histories = {}\n",
    "        reward_histories = {}\n",
    "        \n",
    "        for i in range(env.n_intersections):\n",
    "            obs_histories[i] = deque(maxlen=history_len)\n",
    "            action_histories[i] = deque(maxlen=history_len)\n",
    "            reward_histories[i] = deque(maxlen=history_len)\n",
    "            \n",
    "            dummy_obs = np.zeros((1 + n_neighbors, 25), dtype=np.float32)\n",
    "            dummy_action = np.zeros((1 + n_neighbors, 8), dtype=np.float32)\n",
    "            dummy_reward = np.zeros((1 + n_neighbors, 1), dtype=np.float32)\n",
    "            \n",
    "            for _ in range(history_len):\n",
    "                obs_histories[i].append(dummy_obs.copy())\n",
    "                action_histories[i].append(dummy_action.copy())\n",
    "                reward_histories[i].append(dummy_reward.copy())\n",
    "        \n",
    "        all_actions_onehot = np.zeros((env.n_intersections, 8), dtype=np.float32)\n",
    "        all_actions_onehot[:, 0] = 1.0\n",
    "        all_rewards = np.zeros(env.n_intersections, dtype=np.float32)\n",
    "        \n",
    "        while not done:\n",
    "            actions = []\n",
    "            \n",
    "            for i in range(env.n_intersections):\n",
    "                neighbor_ids = env.get_neighbors(i)\n",
    "                \n",
    "                # Build neighbor data arrays\n",
    "                obs_batch = np.zeros((1 + n_neighbors, 25), dtype=np.float32)\n",
    "                action_batch = np.zeros((1 + n_neighbors, 8), dtype=np.float32)\n",
    "                reward_batch = np.zeros((1 + n_neighbors, 1), dtype=np.float32)\n",
    "                \n",
    "                obs_batch[0] = obs[i]\n",
    "                action_batch[0] = all_actions_onehot[i]\n",
    "                reward_batch[0, 0] = all_rewards[i]\n",
    "                \n",
    "                for idx, nid in enumerate(neighbor_ids):\n",
    "                    if nid >= 0:\n",
    "                        obs_batch[idx + 1] = obs[nid]\n",
    "                        action_batch[idx + 1] = all_actions_onehot[nid]\n",
    "                        reward_batch[idx + 1, 0] = all_rewards[nid]\n",
    "                \n",
    "                obs_histories[i].append(obs_batch.copy())\n",
    "                action_histories[i].append(action_batch.copy())\n",
    "                reward_histories[i].append(reward_batch.copy())\n",
    "                \n",
    "                # Stack histories\n",
    "                obs_hist_array = np.stack(list(obs_histories[i]), axis=0)\n",
    "                act_hist_array = np.stack(list(action_histories[i]), axis=0)\n",
    "                rew_hist_array = np.stack(list(reward_histories[i]), axis=0)\n",
    "                \n",
    "                # Move to device\n",
    "                obs_hist = torch.FloatTensor(obs_hist_array).unsqueeze(0).to(device)\n",
    "                act_hist = torch.FloatTensor(act_hist_array).unsqueeze(0).to(device)\n",
    "                rew_hist = torch.FloatTensor(rew_hist_array).unsqueeze(0).to(device)\n",
    "                current_obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    action_logits, _, _, _ = model(\n",
    "                        obs_hist, act_hist, rew_hist, current_obs_tensor\n",
    "                    )\n",
    "                    action = torch.argmax(action_logits, dim=-1)\n",
    "                \n",
    "                actions.append(action.cpu().item())\n",
    "            \n",
    "            next_obs, rewards_step, done, _ = env.step(actions)\n",
    "            \n",
    "            all_actions_onehot = np.zeros((env.n_intersections, 8), dtype=np.float32)\n",
    "            for i, a in enumerate(actions):\n",
    "                all_actions_onehot[i, a] = 1.0\n",
    "            \n",
    "            all_rewards = rewards_step.astype(np.float32)\n",
    "            obs = next_obs\n",
    "            episode_reward += np.mean(rewards_step)\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"  Eval Episode {ep+1}/{n_episodes}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation Complete!\")\n",
    "    print(f\"  Average Reward: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "    return total_rewards\n",
    "\n",
    "print(\"✓ Evaluation function defined (FIXED with device)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt-.venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
