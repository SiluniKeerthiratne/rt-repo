{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76bec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision numpy matplotlib -q\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a708816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d891eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerTransformer(nn.Module):\n",
    "    \"\"\"Aggregates observations, actions, and rewards from target and neighbors\"\"\"\n",
    "    def __init__(self, d_model=64, nhead=4, num_layers=3, n_neighbors=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        # Separate embeddings for o, a, r\n",
    "        self.obs_embed = nn.Linear(25, d_model)\n",
    "        self.action_embed = nn.Linear(8, d_model)\n",
    "        self.reward_embed = nn.Linear(1, d_model)\n",
    "        \n",
    "        # Decision token\n",
    "        self.decision_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, 3*(1+n_neighbors)+1, d_model))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, obs, actions, rewards, mask=None):\n",
    "        batch_size = obs.shape[0]\n",
    "        \n",
    "        # Embed o, a, r separately\n",
    "        obs_emb = self.obs_embed(obs)\n",
    "        act_emb = self.action_embed(actions)\n",
    "        rew_emb = self.reward_embed(rewards)\n",
    "        \n",
    "        # Concatenate along sequence dimension\n",
    "        seq = torch.stack([obs_emb, act_emb, rew_emb], dim=2)\n",
    "        seq = seq.reshape(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Prepend decision token\n",
    "        decision = self.decision_token.expand(batch_size, -1, -1)\n",
    "        seq = torch.cat([decision, seq], dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        seq = seq + self.pos_embed\n",
    "        \n",
    "        # Apply transformer\n",
    "        out = self.transformer(seq)\n",
    "        \n",
    "        return out[:, 0, :]  # Return decision token output\n",
    "\n",
    "print(\"✓ Lower Transformer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd1f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperTransformer(nn.Module):\n",
    "    \"\"\"Learns scenario-agnostic decision policies across timesteps\"\"\"\n",
    "    def __init__(self, d_model=64, d_output=128, nhead=4, num_layers=3, history_len=10):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_output = d_output\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # Project lower transformer output\n",
    "        self.input_proj = nn.Linear(d_model, d_output)\n",
    "        \n",
    "        # Position embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, history_len, d_output))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_output,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_output*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, lower_outputs):\n",
    "        # Project to d_output dimension\n",
    "        x = self.input_proj(lower_outputs)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Apply transformer\n",
    "        out = self.transformer(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "print(\"✓ Upper Transformer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPredictor(nn.Module):\n",
    "    \"\"\"Predicts next state embedding for learning environment dynamics\"\"\"\n",
    "    def __init__(self, d_model=128, n_neighbors=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_dim = d_model + (1 + n_neighbors) * (8 + 1)\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, prev_embedding, actions, rewards):\n",
    "        batch_size = prev_embedding.shape[0]\n",
    "        \n",
    "        actions_flat = actions.reshape(batch_size, -1)\n",
    "        rewards_flat = rewards.reshape(batch_size, -1)\n",
    "        \n",
    "        x = torch.cat([prev_embedding, actions_flat, rewards_flat], dim=-1)\n",
    "        \n",
    "        return self.predictor(x)\n",
    "\n",
    "print(\"✓ Dynamic Predictor defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232374da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLight(nn.Module):\n",
    "    \"\"\"Complete X-Light model\"\"\"\n",
    "    def __init__(self, \n",
    "                 obs_dim=25,\n",
    "                 action_dim=8,\n",
    "                 d_model=64,\n",
    "                 d_output=128,\n",
    "                 n_neighbors=4,\n",
    "                 history_len=10,\n",
    "                 nhead=4,\n",
    "                 num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.history_len = history_len\n",
    "        \n",
    "        # Lower Transformer\n",
    "        self.lower_transformer = LowerTransformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "        \n",
    "        # Upper Transformer\n",
    "        self.upper_transformer = UpperTransformer(\n",
    "            d_model=d_model,\n",
    "            d_output=d_output,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            history_len=history_len\n",
    "        )\n",
    "        \n",
    "        # Dynamic Predictor\n",
    "        self.dynamic_predictor = DynamicPredictor(\n",
    "            d_model=d_output,\n",
    "            n_neighbors=n_neighbors\n",
    "        )\n",
    "        \n",
    "        # Actor (with residual connection)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(d_output + obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(d_output, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs_history, action_history, reward_history, \n",
    "                current_obs, neighbor_mask=None):\n",
    "        batch_size = obs_history.shape[0]\n",
    "        \n",
    "        # Process each timestep through Lower Transformer\n",
    "        lower_outputs = []\n",
    "        for t in range(self.history_len):\n",
    "            mask_t = neighbor_mask[:, t, :] if neighbor_mask is not None else None\n",
    "            lower_out = self.lower_transformer(\n",
    "                obs_history[:, t, :, :],\n",
    "                action_history[:, t, :, :],\n",
    "                reward_history[:, t, :, :],\n",
    "                mask_t\n",
    "            )\n",
    "            lower_outputs.append(lower_out)\n",
    "        \n",
    "        lower_outputs = torch.stack(lower_outputs, dim=1)\n",
    "        \n",
    "        # Process through Upper Transformer\n",
    "        upper_outputs = self.upper_transformer(lower_outputs)\n",
    "        \n",
    "        # Dynamic prediction for pretext task\n",
    "        predicted_embeddings = []\n",
    "        for t in range(self.history_len - 1):\n",
    "            pred = self.dynamic_predictor(\n",
    "                upper_outputs[:, t, :],\n",
    "                action_history[:, t, :, :],\n",
    "                reward_history[:, t, :, :]\n",
    "            )\n",
    "            predicted_embeddings.append(pred)\n",
    "        predicted_embeddings = torch.stack(predicted_embeddings, dim=1) if predicted_embeddings else None\n",
    "        \n",
    "        # Use last timestep for decision\n",
    "        last_embedding = upper_outputs[:, -1, :]\n",
    "        \n",
    "        # Actor with residual connection\n",
    "        actor_input = torch.cat([last_embedding, current_obs], dim=-1)\n",
    "        action_logits = self.actor(actor_input)\n",
    "        \n",
    "        # Critic\n",
    "        value = self.critic(last_embedding)\n",
    "        \n",
    "        return action_logits, value, predicted_embeddings, upper_outputs[:, 1:, :]\n",
    "\n",
    "print(\"✓ X-Light model defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b074ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTrafficEnv:\n",
    "    \"\"\"Simplified traffic simulation environment\"\"\"\n",
    "    def __init__(self, n_intersections=16, n_neighbors=4, max_queue=50):\n",
    "        self.n_intersections = n_intersections\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.max_queue = max_queue\n",
    "        self.n_phases = 8\n",
    "        \n",
    "        # Create a grid topology (4x4)\n",
    "        self.grid_size = int(np.sqrt(n_intersections))\n",
    "        self.adjacency = self._create_grid_adjacency()\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def _create_grid_adjacency(self):\n",
    "        \"\"\"Create adjacency matrix for grid topology\"\"\"\n",
    "        adj = np.zeros((self.n_intersections, self.n_intersections))\n",
    "        for i in range(self.n_intersections):\n",
    "            row = i // self.grid_size\n",
    "            col = i % self.grid_size\n",
    "            \n",
    "            neighbors = []\n",
    "            if row > 0:\n",
    "                neighbors.append((row-1) * self.grid_size + col)\n",
    "            if row < self.grid_size - 1:\n",
    "                neighbors.append((row+1) * self.grid_size + col)\n",
    "            if col > 0:\n",
    "                neighbors.append(row * self.grid_size + (col-1))\n",
    "            if col < self.grid_size - 1:\n",
    "                neighbors.append(row * self.grid_size + (col+1))\n",
    "            \n",
    "            for n in neighbors:\n",
    "                adj[i, n] = 1\n",
    "                \n",
    "        return adj\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.queues = np.random.randint(0, self.max_queue//2, \n",
    "                                       (self.n_intersections, 4))\n",
    "        self.current_phase = np.zeros(self.n_intersections, dtype=int)\n",
    "        self.phase_duration = np.zeros(self.n_intersections)\n",
    "        self.time_step = 0\n",
    "        \n",
    "        return self._get_all_observations()\n",
    "    \n",
    "    def _get_all_observations(self):\n",
    "        \"\"\"Get observations for all intersections\"\"\"\n",
    "        obs = []\n",
    "        for i in range(self.n_intersections):\n",
    "            obs.append(self._get_observation(i))\n",
    "        return np.array(obs)\n",
    "    \n",
    "    def _get_observation(self, intersection_id):\n",
    "        \"\"\"Get observation for a single intersection\"\"\"\n",
    "        queues = self.queues[intersection_id] / self.max_queue\n",
    "        \n",
    "        phase_onehot = np.zeros(8)\n",
    "        phase_onehot[self.current_phase[intersection_id]] = 1\n",
    "        \n",
    "        occupancy = np.clip(queues + np.random.randn(4) * 0.1, 0, 1)\n",
    "        flow = np.random.rand(4) * 0.5\n",
    "        num_stops = (queues > 0.3).astype(float)\n",
    "        \n",
    "        obs = np.concatenate([queues, phase_onehot, occupancy, flow, num_stops])\n",
    "        return obs\n",
    "    \n",
    "    def get_neighbors(self, intersection_id):\n",
    "        \"\"\"Get neighbor IDs for an intersection\"\"\"\n",
    "        neighbors = np.where(self.adjacency[intersection_id] == 1)[0]\n",
    "        \n",
    "        if len(neighbors) < self.n_neighbors:\n",
    "            neighbors = np.pad(neighbors, (0, self.n_neighbors - len(neighbors)), \n",
    "                             constant_values=-1)\n",
    "        else:\n",
    "            neighbors = neighbors[:self.n_neighbors]\n",
    "            \n",
    "        return neighbors\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute actions for all intersections\"\"\"\n",
    "        rewards = np.zeros(self.n_intersections)\n",
    "        \n",
    "        for i in range(self.n_intersections):\n",
    "            if actions[i] != self.current_phase[i]:\n",
    "                self.current_phase[i] = actions[i]\n",
    "                self.phase_duration[i] = 0\n",
    "            else:\n",
    "                self.phase_duration[i] += 1\n",
    "            \n",
    "            # Simulate traffic flow\n",
    "            active_movements = self._get_active_movements(actions[i])\n",
    "            for movement in active_movements:\n",
    "                reduction = np.random.poisson(3)\n",
    "                self.queues[i, movement] = max(0, self.queues[i, movement] - reduction)\n",
    "            \n",
    "            arrivals = np.random.poisson(2, 4)\n",
    "            self.queues[i] += arrivals\n",
    "            self.queues[i] = np.clip(self.queues[i], 0, self.max_queue)\n",
    "            \n",
    "            queue_penalty = -np.sum(self.queues[i])\n",
    "            wait_penalty = -np.sum(self.queues[i] ** 1.5) * 0.01\n",
    "            rewards[i] = queue_penalty + wait_penalty\n",
    "        \n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= 360  # Shorter episodes for faster training\n",
    "        \n",
    "        obs = self._get_all_observations()\n",
    "        \n",
    "        return obs, rewards, done, {}\n",
    "    \n",
    "    def _get_active_movements(self, phase):\n",
    "        \"\"\"Get which movements are active for a phase\"\"\"\n",
    "        phase_movements = {\n",
    "            0: [0, 2], 1: [1, 3], 2: [0, 1], 3: [0, 3],\n",
    "            4: [2, 1], 5: [2, 3], 6: [0], 7: [1],\n",
    "        }\n",
    "        return phase_movements.get(phase, [])\n",
    "\n",
    "print(\"✓ Traffic Environment defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb422bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    \"\"\"Store trajectories for PPO training\"\"\"\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        \n",
    "    def push(self, obs_history, action_history, reward_history, \n",
    "             current_obs, action, reward, value, log_prob, neighbor_indices):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "            \n",
    "        self.buffer.append({\n",
    "            'obs_history': obs_history,\n",
    "            'action_history': action_history,\n",
    "            'reward_history': reward_history,\n",
    "            'current_obs': current_obs,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'value': value,\n",
    "            'log_prob': log_prob,\n",
    "            'neighbor_indices': neighbor_indices\n",
    "        })\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "    \n",
    "    def clear(self):\n",
    "        self.buffer = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def compute_ppo_loss(model, batch, clip_epsilon=0.2, alpha=1.0, beta=0.5, gamma=1.0):\n",
    "    \"\"\"Compute PPO loss with dynamic prediction\"\"\"\n",
    "    obs_hist = torch.stack([torch.FloatTensor(t['obs_history']) for t in batch])\n",
    "    act_hist = torch.stack([torch.FloatTensor(t['action_history']) for t in batch])\n",
    "    rew_hist = torch.stack([torch.FloatTensor(t['reward_history']) for t in batch])\n",
    "    current_obs = torch.stack([torch.FloatTensor(t['current_obs']) for t in batch])\n",
    "    actions = torch.LongTensor([t['action'] for t in batch])\n",
    "    rewards = torch.FloatTensor([t['reward'] for t in batch])\n",
    "    old_values = torch.FloatTensor([t['value'] for t in batch])\n",
    "    old_log_probs = torch.FloatTensor([t['log_prob'] for t in batch])\n",
    "    \n",
    "    # Forward pass\n",
    "    action_logits, values, predicted_embs, target_embs = model(\n",
    "        obs_hist, act_hist, rew_hist, current_obs\n",
    "    )\n",
    "    \n",
    "    # Actor loss (PPO)\n",
    "    dist = Categorical(logits=action_logits)\n",
    "    new_log_probs = dist.log_prob(actions)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    advantages = rewards - old_values.detach()\n",
    "    \n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages\n",
    "    actor_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    # Critic loss\n",
    "    critic_loss = F.mse_loss(values.squeeze(), rewards)\n",
    "    \n",
    "    # Dynamic prediction loss\n",
    "    if predicted_embs is not None and target_embs is not None:\n",
    "        pred_loss = F.mse_loss(predicted_embs, target_embs.detach())\n",
    "    else:\n",
    "        pred_loss = torch.tensor(0.0)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = alpha * actor_loss + beta * critic_loss + gamma * pred_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'actor_loss': actor_loss.item(),\n",
    "        'critic_loss': critic_loss.item(),\n",
    "        'pred_loss': pred_loss.item() if isinstance(pred_loss, torch.Tensor) else pred_loss,\n",
    "        'entropy': entropy.item()\n",
    "    }\n",
    "\n",
    "print(\"✓ Training components defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dbd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XLightTrainer:\n",
    "    \"\"\"Trainer for X-Light model\"\"\"\n",
    "    def __init__(self, model, env, lr=3e-4, history_len=10, n_neighbors=4):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.history_len = history_len\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.buffer = RolloutBuffer(capacity=10000)\n",
    "        \n",
    "        self.obs_histories = {}\n",
    "        self.action_histories = {}\n",
    "        self.reward_histories = {}\n",
    "        \n",
    "        self._init_histories()\n",
    "    \n",
    "    def _init_histories(self):\n",
    "        \"\"\"Initialize history buffers\"\"\"\n",
    "        for i in range(self.env.n_intersections):\n",
    "            self.obs_histories[i] = deque(maxlen=self.history_len)\n",
    "            self.action_histories[i] = deque(maxlen=self.history_len)\n",
    "            self.reward_histories[i] = deque(maxlen=self.history_len)\n",
    "            \n",
    "            dummy_obs = np.zeros((1 + self.n_neighbors, 25))\n",
    "            dummy_action = np.zeros((1 + self.n_neighbors, 8))\n",
    "            dummy_reward = np.zeros((1 + self.n_neighbors, 1))\n",
    "            \n",
    "            for _ in range(self.history_len):\n",
    "                self.obs_histories[i].append(dummy_obs)\n",
    "                self.action_histories[i].append(dummy_action)\n",
    "                self.reward_histories[i].append(dummy_reward)\n",
    "    \n",
    "    def _get_neighbor_data(self, intersection_id, all_obs, all_actions, all_rewards):\n",
    "        \"\"\"Get data for target and neighbors\"\"\"\n",
    "        neighbor_ids = self.env.get_neighbors(intersection_id)\n",
    "        \n",
    "        obs_batch = [all_obs[intersection_id]]\n",
    "        action_batch = [all_actions[intersection_id]]\n",
    "        reward_batch = [[all_rewards[intersection_id]]]\n",
    "        \n",
    "        for nid in neighbor_ids:\n",
    "            if nid >= 0:\n",
    "                obs_batch.append(all_obs[nid])\n",
    "                action_batch.append(all_actions[nid])\n",
    "                reward_batch.append([all_rewards[nid]])\n",
    "            else:\n",
    "                obs_batch.append(np.zeros_like(all_obs[0]))\n",
    "                action_batch.append(np.zeros(8))\n",
    "                reward_batch.append([0.0])\n",
    "        \n",
    "        return (np.array(obs_batch), \n",
    "                np.array(action_batch), \n",
    "                np.array(reward_batch),\n",
    "                neighbor_ids)\n",
    "    \n",
    "    def train_episode(self):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        all_actions_onehot = np.zeros((self.env.n_intersections, 8))\n",
    "        all_actions_onehot[:, 0] = 1\n",
    "        all_rewards = np.zeros(self.env.n_intersections)\n",
    "        \n",
    "        while not done:\n",
    "            actions = []\n",
    "            \n",
    "            for i in range(self.env.n_intersections):\n",
    "                obs_data, action_data, reward_data, neighbor_ids = \\\n",
    "                    self._get_neighbor_data(i, obs, all_actions_onehot, all_rewards)\n",
    "                \n",
    "                self.obs_histories[i].append(obs_data)\n",
    "                self.action_histories[i].append(action_data)\n",
    "                self.reward_histories[i].append(reward_data)\n",
    "                \n",
    "                obs_hist = torch.FloatTensor(np.array(self.obs_histories[i])).unsqueeze(0)\n",
    "                act_hist = torch.FloatTensor(np.array(self.action_histories[i])).unsqueeze(0)\n",
    "                rew_hist = torch.FloatTensor(np.array(self.reward_histories[i])).unsqueeze(0)\n",
    "                current_obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    action_logits, value, _, _ = self.model(\n",
    "                        obs_hist, act_hist, rew_hist, current_obs_tensor\n",
    "                    )\n",
    "                    dist = Categorical(logits=action_logits)\n",
    "                    action = dist.sample()\n",
    "                    log_prob = dist.log_prob(action)\n",
    "                \n",
    "                actions.append(action.item())\n",
    "                \n",
    "                self.buffer.push(\n",
    "                    obs_history=np.array(self.obs_histories[i]),\n",
    "                    action_history=np.array(self.action_histories[i]),\n",
    "                    reward_history=np.array(self.reward_histories[i]),\n",
    "                    current_obs=obs[i],\n",
    "                    action=action.item(),\n",
    "                    reward=0,\n",
    "                    value=value.item(),\n",
    "                    log_prob=log_prob.item(),\n",
    "                    neighbor_indices=neighbor_ids\n",
    "                )\n",
    "            \n",
    "            next_obs, rewards, done, _ = self.env.step(actions)\n",
    "            \n",
    "            for i in range(self.env.n_intersections):\n",
    "                if len(self.buffer) > i:\n",
    "                    self.buffer.buffer[-(self.env.n_intersections - i)]['reward'] = rewards[i]\n",
    "            \n",
    "            all_actions_onehot = np.zeros((self.env.n_intersections, 8))\n",
    "            for i, a in enumerate(actions):\n",
    "                all_actions_onehot[i, a] = 1\n",
    "            \n",
    "            all_rewards = rewards\n",
    "            obs = next_obs\n",
    "            episode_reward += np.mean(rewards)\n",
    "            step_count += 1\n",
    "        \n",
    "        return episode_reward, step_count\n",
    "    \n",
    "    def update(self, batch_size=64, n_updates=4):\n",
    "        \"\"\"Update model using PPO\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return {}\n",
    "        \n",
    "        losses = {\n",
    "            'actor_loss': [],\n",
    "            'critic_loss': [],\n",
    "            'pred_loss': [],\n",
    "            'entropy': []\n",
    "        }\n",
    "        \n",
    "        for _ in range(n_updates):\n",
    "            batch = self.buffer.sample(batch_size)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss, loss_dict = compute_ppo_loss(self.model, batch)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            for key in loss_dict:\n",
    "                losses[key].append(loss_dict[key])\n",
    "        \n",
    "        avg_losses = {k: np.mean(v) for k, v in losses.items()}\n",
    "        return avg_losses\n",
    "\n",
    "print(\"✓ Trainer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xlight(n_episodes=50, update_interval=5, plot_live=True):\n",
    "    \"\"\"Main training loop with live plotting\"\"\"\n",
    "    \n",
    "    # Create environment\n",
    "    env = SimpleTrafficEnv(n_intersections=16, n_neighbors=4)\n",
    "    \n",
    "    # Create model\n",
    "    model = XLight(\n",
    "        obs_dim=25,\n",
    "        action_dim=8,\n",
    "        d_model=64,\n",
    "        d_output=128,\n",
    "        n_neighbors=4,\n",
    "        history_len=10,\n",
    "        nhead=4,\n",
    "        num_layers=3\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = XLightTrainer(model, env, lr=3e-4)\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        # Train episode\n",
    "        episode_reward, steps = trainer.train_episode()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Update model\n",
    "        if episode % update_interval == 0 and episode > 0:\n",
    "            losses = trainer.update(batch_size=64, n_updates=4)\n",
    "            \n",
    "            if losses:\n",
    "                actor_losses.append(losses['actor_loss'])\n",
    "                critic_losses.append(losses['critic_loss'])\n",
    "            \n",
    "            print(f\"Episode {episode}/{n_episodes} | Reward: {episode_reward:.2f} | Steps: {steps}\")\n",
    "            if losses:\n",
    "                print(f\"  Actor Loss: {losses['actor_loss']:.4f} | Critic Loss: {losses['critic_loss']:.4f}\")\n",
    "                print(f\"  Pred Loss: {losses['pred_loss']:.4f} | Entropy: {losses['entropy']:.4f}\")\n",
    "            \n",
    "            # Live plotting\n",
    "            if plot_live and episode > update_interval:\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "                \n",
    "                # Plot rewards\n",
    "                axes[0].plot(episode_rewards, label='Episode Reward', color='blue', alpha=0.6)\n",
    "                axes[0].plot(np.convolve(episode_rewards, np.ones(5)/5, mode='valid'), \n",
    "                           label='Moving Avg (5)', color='red', linewidth=2)\n",
    "                axes[0].set_xlabel('Episode')\n",
    "                axes[0].set_ylabel('Average Reward')\n",
    "                axes[0].set_title('Training Progress')\n",
    "                axes[0].legend()\n",
    "                axes[0].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Plot losses\n",
    "                if actor_losses:\n",
    "                    x_loss = np.arange(len(actor_losses)) * update_interval\n",
    "                    axes[1].plot(x_loss, actor_losses, label='Actor Loss', color='orange')\n",
    "                    axes[1].plot(x_loss, critic_losses, label='Critic Loss', color='green')\n",
    "                    axes[1].set_xlabel('Episode')\n",
    "                    axes[1].set_ylabel('Loss')\n",
    "                    axes[1].set_title('Training Losses')\n",
    "                    axes[1].legend()\n",
    "                    axes[1].grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                print(f\"\\nEpisode {episode}/{n_episodes}\")\n",
    "                print(f\"Latest Reward: {episode_reward:.2f}\")\n",
    "                print(f\"Average Reward (last 10): {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "            \n",
    "            # Clear buffer periodically\n",
    "            if episode % (update_interval * 4) == 0:\n",
    "                trainer.buffer.clear()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training completed!\")\n",
    "    \n",
    "    return model, trainer, episode_rewards\n",
    "\n",
    "# This cell just defines the function, don't run training yet\n",
    "print(\"✓ Training function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe15c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL TO START TRAINING\n",
    "model, trainer, rewards = train_xlight(n_episodes=50, update_interval=5, plot_live=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14d1991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards, label='Episode Reward', alpha=0.6)\n",
    "plt.plot(np.convolve(rewards, np.ones(5)/5, mode='valid'), \n",
    "         label='Moving Average (5)', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Training Progress - Final')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Reward Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Statistics:\")\n",
    "print(f\"  Mean Reward: {np.mean(rewards):.2f}\")\n",
    "print(f\"  Std Reward: {np.std(rewards):.2f}\")\n",
    "print(f\"  Best Reward: {np.max(rewards):.2f}\")\n",
    "print(f\"  Worst Reward: {np.min(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96314e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=5):\n",
    "    \"\"\"Evaluate trained model\"\"\"\n",
    "    print(\"Evaluating model...\")\n",
    "    total_rewards = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Initialize histories\n",
    "        obs_histories = {}\n",
    "        action_histories = {}\n",
    "        reward_histories = {}\n",
    "        \n",
    "        for i in range(env.n_intersections):\n",
    "            obs_histories[i] = deque(maxlen=10)\n",
    "            action_histories[i] = deque(maxlen=10)\n",
    "            reward_histories[i] = deque(maxlen=10)\n",
    "            \n",
    "            dummy_obs = np.zeros((1 + 4, 25))\n",
    "            dummy_action = np.zeros((1 + 4, 8))\n",
    "            dummy_reward = np.zeros((1 + 4, 1))\n",
    "            \n",
    "            for _ in range(10):\n",
    "                obs_histories[i].append(dummy_obs)\n",
    "                action_histories[i].append(dummy_action)\n",
    "                reward_histories[i].append(dummy_reward)\n",
    "        \n",
    "        all_actions_onehot = np.zeros((env.n_intersections, 8))\n",
    "        all_actions_onehot[:, 0] = 1\n",
    "        all_rewards = np.zeros(env.n_intersections)\n",
    "        \n",
    "        while not done:\n",
    "            actions = []\n",
    "            \n",
    "            for i in range(env.n_intersections):\n",
    "                neighbor_ids = env.get_neighbors(i)\n",
    "                \n",
    "                obs_batch = [obs[i]]\n",
    "                action_batch = [all_actions_onehot[i]]\n",
    "                reward_batch = [[all_rewards[i]]]\n",
    "                \n",
    "                for nid in neighbor_ids:\n",
    "                    if nid >= 0:\n",
    "                        obs_batch.append(obs[nid])\n",
    "                        action_batch.append(all_actions_onehot[nid])\n",
    "                        reward_batch.append([all_rewards[nid]])\n",
    "                    else:\n",
    "                        obs_batch.append(np.zeros_like(obs[0]))\n",
    "                        action_batch.append(np.zeros(8))\n",
    "                        reward_batch.append([0.0])\n",
    "                \n",
    "                obs_data = np.array(obs_batch)\n",
    "                action_data = np.array(action_batch)\n",
    "                reward_data = np.array(reward_batch)\n",
    "                \n",
    "                obs_histories[i].append(obs_data)\n",
    "                action_histories[i].append(action_data)\n",
    "                reward_histories[i].append(reward_data)\n",
    "                \n",
    "                obs_hist = torch.FloatTensor(np.array(obs_histories[i])).unsqueeze(0)\n",
    "                act_hist = torch.FloatTensor(np.array(action_histories[i])).unsqueeze(0)\n",
    "                rew_hist = torch.FloatTensor(np.array(reward_histories[i])).unsqueeze(0)\n",
    "                current_obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    action_logits, _, _, _ = model(\n",
    "                        obs_hist, act_hist, rew_hist, current_obs_tensor\n",
    "                    )\n",
    "                    action = torch.argmax(action_logits, dim=-1)\n",
    "                \n",
    "                actions.append(action.item())\n",
    "            \n",
    "            next_obs, rewards_step, done, _ = env.step(actions)\n",
    "            \n",
    "            all_actions_onehot = np.zeros((env.n_intersections, 8))\n",
    "            for i, a in enumerate(actions):\n",
    "                all_actions_onehot[i, a] = 1\n",
    "            \n",
    "            all_rewards = rewards_step\n",
    "            obs = next_obs\n",
    "            episode_reward += np.mean(rewards_step)\n",
    "        \n",
    "        total_rewards.append(episode_reward)\n",
    "        print(f\"  Eval Episode {ep+1}/{n_episodes}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    print(f\"\\n✓ Evaluation Complete!\")\n",
    "    print(f\"  Average Reward: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "    return total_rewards\n",
    "\n",
    "# Run evaluation\n",
    "eval_rewards = evaluate_model(model, trainer.env, n_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rt-.venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
